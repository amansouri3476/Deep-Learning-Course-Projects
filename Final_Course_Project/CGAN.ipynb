{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "I6W5Yw31w2_s",
        "colab_type": "code",
        "outputId": "8253a3e3-4fcd-40fc-8c4b-8ce7f77aed1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HxXwdWJLp7Zh",
        "colab_type": "code",
        "outputId": "5a7e7fa6-84a4-4c4a-f2e2-8d0dad105530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RRFbLrR432qp",
        "colab_type": "code",
        "outputId": "afba3016-9f76-4253-dcf2-5f0cc47c1a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "cd content/gdrive/'My Drive'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5isMhskW4GEZ",
        "colab_type": "code",
        "outputId": "ecce51d8-73fc-4501-9509-2dc9efd0290b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "cd train_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/train_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N7rhQv78WerG",
        "colab_type": "code",
        "outputId": "d01205d2-1399-44ff-97b3-4a91727bca58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "38gU2sL-_Sq2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Python 2.7\n",
        "# By: Amin Mansouri\n",
        "\n",
        "# Import libraries\n",
        "import glob\n",
        "import os, time, itertools, imageio, pickle, random\n",
        "import imageio as imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "def png2np(im_path):\n",
        "    images = []\n",
        "    for im_path in glob.glob(im_path):\n",
        "        img = imageio.imread(im_path)\n",
        "        # img = np.reshape(im_filtered, newshape=784)\n",
        "        images.append(img)\n",
        "    images = np.array(images)\n",
        "    return images\n",
        "\n",
        "\n",
        "Char2idx = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12,\n",
        "            'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24,\n",
        "            'Z': 25}\n",
        "\n",
        "idx2char = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M',\n",
        "            13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y',\n",
        "            25: 'Z'}\n",
        "\n",
        "# leaky_relu\n",
        "def lrelu(X, leak=0.2):\n",
        "    f1 = 0.5 * (1 + leak)\n",
        "    f2 = 0.5 * (1 - leak)\n",
        "    return f1 * X + f2 * tf.abs(X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VcCZ6QmP7Cu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# G(z)\n",
        "def generator(x, y_label, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('generator', reuse=reuse):\n",
        "        # initializer\n",
        "        w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        b_init = tf.constant_initializer(0.0)\n",
        "\n",
        "        # concat layer\n",
        "        cat1 = tf.concat([x, y_label], 3)\n",
        "#         print(\"generator\", cat1.get_shape())\n",
        "        # 1st hidden layer\n",
        "        deconv1 = tf.layers.conv2d_transpose(cat1, 256, [8, 8], strides=(1, 1), padding='valid', kernel_initializer=w_init, bias_initializer=b_init)\n",
        "        lrelu1 = lrelu(tf.layers.batch_normalization(deconv1, training=isTrain), 0.2)\n",
        "#         print(\"lrelu1\", lrelu1.get_shape())\n",
        "        # 2nd hidden layer\n",
        "        deconv2 = tf.layers.conv2d_transpose(lrelu1, 128, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
        "        lrelu2 = lrelu(tf.layers.batch_normalization(deconv2, training=isTrain), 0.2)\n",
        "#         print(\"lrelu2\", lrelu2.get_shape())\n",
        "        # output layer\n",
        "        deconv3 = tf.layers.conv2d_transpose(lrelu2, 1, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
        "        o = tf.nn.tanh(deconv3)\n",
        "#         print(\"o\", o.get_shape())\n",
        "        return o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h1f7QRAc8jZM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# D(x)\n",
        "def discriminator(x, y_fill, isTrain=True, reuse=False):\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "        # initializer\n",
        "        w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        b_init = tf.constant_initializer(0.0)\n",
        "\n",
        "        # concat layer\n",
        "        cat1 = tf.concat([x, y_fill], 3)\n",
        "#         print(\"discriminator\", cat1.get_shape())\n",
        "        # 1st hidden layer\n",
        "        conv1 = tf.layers.conv2d(cat1, 128, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
        "        lrelu1 = lrelu(conv1, 0.2)\n",
        "#         print(\"lrelu1\", lrelu1.get_shape())\n",
        "        # 2nd hidden layer\n",
        "        conv2 = tf.layers.conv2d(lrelu1, 256, [5, 5], strides=(2, 2), padding='same', kernel_initializer=w_init, bias_initializer=b_init)\n",
        "        lrelu2 = lrelu(tf.layers.batch_normalization(conv2, training=isTrain), 0.2)\n",
        "#         print(\"lrelu2\", lrelu2.get_shape())\n",
        "        # output layer\n",
        "        conv3 = tf.layers.conv2d(lrelu2, 1, [8, 8], strides=(1, 1), padding='valid', kernel_initializer=w_init)\n",
        "        o = tf.nn.sigmoid(conv3)\n",
        "#         print(\"o\", o.get_shape())\n",
        "        return o, conv3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3V7VHVi5rkYO",
        "colab_type": "code",
        "outputId": "e1b01897-1c4a-459f-ee7a-d589d3a192a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# preprocess\n",
        "img_size = 32\n",
        "n_class = 26\n",
        "onehot = np.eye(n_class)\n",
        "temp_z_ = np.random.normal(0, 1, (n_class, 1, 1, 100))\n",
        "fixed_z_ = temp_z_\n",
        "fixed_y_ = np.zeros((n_class, 1))\n",
        "for i in range(n_class - 1):\n",
        "    fixed_z_ = np.concatenate([fixed_z_, temp_z_], 0)\n",
        "    temp = np.ones((n_class, 1)) + i\n",
        "    fixed_y_ = np.concatenate([fixed_y_, temp], 0)\n",
        "print(fixed_y_.shape)\n",
        "# print(fixed_y_.astype(np.int32))\n",
        "fixed_y_ = onehot[fixed_y_.astype(np.int32)].reshape((n_class * n_class, 1, 1, n_class))\n",
        "# print(fixed_y_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(676, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Rn2wu2f8nSw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
        "    test_images = sess.run(G_z, {z: fixed_z_, y_label: fixed_y_, isTrain: False})\n",
        "\n",
        "    size_figure_grid = 26\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(8, 8))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(26*26):\n",
        "        i = k // 26\n",
        "        j = k % 26\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (img_size, img_size)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sr09GBE78pgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4M2ldJwX8szd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training parameters\n",
        "batch_size = 100\n",
        "# lr = 0.0002\n",
        "train_epoch = 10\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lr = tf.train.exponential_decay(0.0002, global_step, 500, 0.95, staircase=True)\n",
        "import pickle\n",
        "pickle_in = open(\"train_data.pickle\", \"rb\")\n",
        "train_data = pickle.load(pickle_in)\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(train_data['data'], train_data['labels'], test_size=0.2)\n",
        "\n",
        "train_X = (x_train.reshape(-1, 32, 32, 1))\n",
        "test_X = (x_validation.reshape(-1, 32, 32, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0bxy0VQob62x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X = (train_X)/(np.max(train_X))\n",
        "test_X = (test_X)/(np.max(train_X))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5eGNYL5CZTsq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X = (train_X)/(np.std(train_X))\n",
        "test_X = (test_X)/(np.std(train_X))\n",
        "\n",
        "# min_max_scaler = preprocessing.MinMaxScaler()\n",
        "# X_train_minmax = min_max_scaler.fit_transform(train_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b6_oJHKWZZxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# variables : input\n",
        "x = tf.placeholder(tf.float32, shape=(None, img_size, img_size, 1))\n",
        "z = tf.placeholder(tf.float32, shape=(None, 1, 1, 100))\n",
        "y_label = tf.placeholder(tf.float32, shape=(None, 1, 1, 26))\n",
        "y_fill = tf.placeholder(tf.float32, shape=(None, img_size, img_size, 26))\n",
        "isTrain = tf.placeholder(dtype=tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Y7q3Tou8wc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# networks : generator\n",
        "G_z = generator(z, y_label, isTrain, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "# networks : discriminator\n",
        "D_real, D_real_logits = discriminator(x, y_fill, isTrain, reuse=tf.AUTO_REUSE)\n",
        "D_fake, D_fake_logits = discriminator(G_z, y_fill, isTrain, reuse=tf.AUTO_REUSE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMZ-8Gz58yzi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss for each network\n",
        "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones([batch_size, 1, 1, 1])))\n",
        "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros([batch_size, 1, 1, 1])))\n",
        "D_loss = D_loss_real + D_loss_fake\n",
        "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones([batch_size, 1, 1, 1])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B_ulHcxz8070",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# trainable variables for each network\n",
        "T_vars = tf.trainable_variables()\n",
        "D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
        "G_vars = [var for var in T_vars if var.name.startswith('generator')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ei8UgChH84YD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizer for each network\n",
        "\n",
        "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "    optim = tf.train.AdamOptimizer(lr, beta1=0.5)\n",
        "    D_optim = optim.minimize(D_loss, global_step=global_step, var_list=D_vars)\n",
        "    # D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(D_loss, var_list=D_vars)\n",
        "    G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(G_loss, var_list=G_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XYQe_I4865X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# open session and initialize all variables\n",
        "sess = tf.InteractiveSession()\n",
        "tf.global_variables_initializer().run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XCxNEFi28-Oo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set = train_X\n",
        "train_label = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFYAv3Yg9A-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# results save folder\n",
        "root = 'MNIST_cDCGAN_results/'\n",
        "model = 'MNIST_cDCGAN_'\n",
        "if not os.path.isdir(root):\n",
        "    os.mkdir(root)\n",
        "if not os.path.isdir(root + 'Fixed_results'):\n",
        "    os.mkdir(root + 'Fixed_results')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFWtYAXY9SQZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_hist = {}\n",
        "train_hist['D_losses'] = []\n",
        "train_hist['G_losses'] = []\n",
        "train_hist['per_epoch_ptimes'] = []\n",
        "train_hist['total_ptime'] = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVKo1DLzXuaN",
        "colab_type": "code",
        "outputId": "d67130e8-90fb-4fa3-ee08-ff6651261d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "shuffle_idxs = random.sample(range(0, train_set.shape[0]), train_set.shape[0])\n",
        "shuffled_set = train_set[shuffle_idxs]\n",
        "shuffled_label = train_label[shuffle_idxs]\n",
        "x_ = shuffled_set[0*batch_size:(0+1)*batch_size]\n",
        "iter = 0\n",
        "y_label_ = shuffled_label[iter*batch_size:(iter+1)*batch_size].reshape([batch_size, 1, 1, 26])\n",
        "print(y_label.shape)\n",
        "y_fill_ = y_label_ * np.ones([batch_size, img_size, img_size, 26])\n",
        "print(y_fill.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 1, 1, 26)\n",
            "(?, 32, 32, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1RXrfIgH9Z5L",
        "colab_type": "code",
        "outputId": "4d48ce10-9aa2-4b2c-cae6-a121d041f252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "cell_type": "code",
      "source": [
        "# training-loop\n",
        "np.random.seed(int(time.time()))\n",
        "print('training start!')\n",
        "start_time = time.time()\n",
        "for epoch in range(10):\n",
        "    G_losses = []\n",
        "    D_losses = []\n",
        "    epoch_start_time = time.time()\n",
        "    shuffle_idxs = random.sample(range(0, train_set.shape[0]), train_set.shape[0])\n",
        "    shuffled_set = train_set[shuffle_idxs]\n",
        "    shuffled_label = train_label[shuffle_idxs]\n",
        "    for iter in tqdm(range(shuffled_set.shape[0] // batch_size)):\n",
        "        # update discriminator\n",
        "        x_ = shuffled_set[iter*batch_size:(iter+1)*batch_size]\n",
        "#         print(x_.shape)\n",
        "        y_label_ = shuffled_label[iter*batch_size:(iter+1)*batch_size].reshape([batch_size, 1, 1, 26])\n",
        "#         print(y_label.shape)\n",
        "        y_fill_ = y_label_ * np.ones([batch_size, img_size, img_size, 26])\n",
        "#         print(y_fill.shape)\n",
        "        z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
        "#         print(z_.shape)\n",
        "        loss_d_, _ = sess.run([D_loss, D_optim], {x: x_, z: z_, y_fill: y_fill_, y_label: y_label_, isTrain: True})\n",
        "#         print(\"hello\")\n",
        "        # update generator\n",
        "        z_ = np.random.normal(0, 1, (batch_size, 1, 1, 100))\n",
        "        y_ = np.random.randint(0, 25, (batch_size, 1))\n",
        "        y_label_ = onehot[y_.astype(np.int32)].reshape([batch_size, 1, 1, 26])\n",
        "        y_fill_ = y_label_ * np.ones([batch_size, img_size, img_size, 26])\n",
        "        loss_g_, _ = sess.run([G_loss, G_optim], {z: z_, x: x_, y_fill: y_fill_, y_label: y_label_, isTrain: True})\n",
        "\n",
        "        errD_fake = D_loss_fake.eval({z: z_, y_label: y_label_, y_fill: y_fill_, isTrain: False})\n",
        "        errD_real = D_loss_real.eval({x: x_, y_label: y_label_, y_fill: y_fill_, isTrain: False})\n",
        "        errG = G_loss.eval({z: z_, y_label: y_label_, y_fill: y_fill_, isTrain: False})\n",
        "\n",
        "        D_losses.append(errD_fake + errD_real)\n",
        "        G_losses.append(errG)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "    print('[%d/%d] - ptime: %.2f loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, np.mean(D_losses), np.mean(G_losses)))\n",
        "    fixed_p = root + 'Fixed_results/' + model + str(epoch + 1) + '.png'\n",
        "    show_result((epoch + 1), save=True, path=fixed_p)\n",
        "    train_hist['D_losses'].append(np.mean(D_losses))\n",
        "    train_hist['G_losses'].append(np.mean(G_losses))\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "end_time = time.time()\n",
        "total_ptime = end_time - start_time\n",
        "train_hist['total_ptime'].append(total_ptime)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:32<00:00,  3.26it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1/10] - ptime: 513.58 loss_d: 1.282, loss_g: 1.145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:31<00:00,  3.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2/10] - ptime: 512.46 loss_d: 0.803, loss_g: 1.347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:31<00:00,  3.24it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[3/10] - ptime: 512.41 loss_d: 0.538, loss_g: 1.545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:35<00:00,  3.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[4/10] - ptime: 515.97 loss_d: 0.367, loss_g: 1.838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:31<00:00,  3.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[5/10] - ptime: 512.24 loss_d: 0.299, loss_g: 1.927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:38<00:00,  3.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[6/10] - ptime: 519.18 loss_d: 0.253, loss_g: 2.023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:37<00:00,  3.22it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[7/10] - ptime: 518.31 loss_d: 0.234, loss_g: 2.030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:36<00:00,  3.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[8/10] - ptime: 517.64 loss_d: 0.208, loss_g: 2.086\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:34<00:00,  3.23it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[9/10] - ptime: 514.76 loss_d: 0.190, loss_g: 2.102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1664/1664 [08:33<00:00,  3.27it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[10/10] - ptime: 513.75 loss_d: 0.178, loss_g: 2.118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HejoS7vvV30h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Avg per epoch ptime: %.2f, total %d epochs ptime: %.2f' % (np.mean(train_hist['per_epoch_ptimes']), train_epoch, total_ptime))\n",
        "print(\"Training finish!... save training results\")\n",
        "with open(root + model + 'train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path=root + model + 'train_hist.png')\n",
        "\n",
        "images = []\n",
        "for e in range(train_epoch):\n",
        "    img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
        "    images.append(imageio.imread(img_name))\n",
        "imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)\n",
        "\n",
        "sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZLhPLrSOsxQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}