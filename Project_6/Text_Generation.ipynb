{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RY8CEIM75R2V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## This piece of code is used to enable using tf.random.categorical"
      ]
    },
    {
      "metadata": {
        "id": "MxainIwGlZMK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tf-nightly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6UPL1AegfTm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUFJBVvn559M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## First, we read the text data and then creat its corresponding vocabulary containig its unique characters. After that two mappings are introduced to convert each character to its index in the dictionary and vice versa.\n",
        "## Last line has the responsibility to convert the whole text to a its index form, i.e. each char is represented by a number in text_as_int."
      ]
    },
    {
      "metadata": {
        "id": "87B9fRr4lZPG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = open('Book.txt').read()\n",
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1bxgKlrF8A39",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Here we define 40 character window for each sentence and number of examples per epoch based on that"
      ]
    },
    {
      "metadata": {
        "id": "oACp9V_2m0_S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 40\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9G8TfcM82JC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The batch method lets us easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "metadata": {
        "id": "6yU1FIl8nP48",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "80kI4eN09quD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## For each sequence, I duplicate and shift it to form the input and target text by using the map to apply a function to each batch:"
      ]
    },
    {
      "metadata": {
        "id": "gXeJyBvGnnWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1huCDKGdAihj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Shuffling and packing data into batches"
      ]
    },
    {
      "metadata": {
        "id": "7z4UbJEvpRbT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDJ0vfH7A4G0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the model using Keras and testing whether there is GPU available (since I am using Google Colab)\n",
        "## I use GRU with sigmoid activation function."
      ]
    },
    {
      "metadata": {
        "id": "8Q3TTynOpqzb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 128\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4dwVOqhmBYnz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## tf.keras.layers.Dense: The output layer, with vocab_size outputs."
      ]
    },
    {
      "metadata": {
        "id": "HTlj9nvFqB0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8epyD4pNBxX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next character"
      ]
    },
    {
      "metadata": {
        "id": "k37me_FnqFlA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "dee82f46-25e7-4945-89ad-db2dcc7d6c18"
      },
      "cell_type": "code",
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jsn51ASaqPhm",
        "colab_type": "code",
        "outputId": "cee1ed4f-d017-466d-af47-316595f19425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           19712     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 128)           147840    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 77)            9933      \n",
            "=================================================================\n",
            "Total params: 177,485\n",
            "Trainable params: 177,485\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K2Cr4_txCMPo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "We use the previous RNN state, and the input of this time step to predict the class of the next character.\n",
        "\n",
        "We also define the loss function."
      ]
    },
    {
      "metadata": {
        "id": "rCHUNHaIrI8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9IQuTcaDGKW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Configuring the training procedure using the Model.compile method"
      ]
    },
    {
      "metadata": {
        "id": "Fyvwe16vrQNZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "model.compile(optimizer = tf.train.RMSPropOptimizer(learning_rate), loss = loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k0Cw_UbFDVOV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Saving Checkpoints during training."
      ]
    },
    {
      "metadata": {
        "id": "X3KAEpn2rVtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32jMcSVNrhIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPOCHS=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMnRlnqermgo",
        "colab_type": "code",
        "outputId": "e1fec033-7545-4aee-81e7-1677117e723a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:1730: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "Epoch 1/20\n",
            "267/267==============================] - 73s 275ms/step - loss: 2.8110\n",
            "Epoch 2/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.6108\n",
            "Epoch 3/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.5098\n",
            "Epoch 4/20\n",
            "267/267==============================] - 71s 266ms/step - loss: 1.4781\n",
            "Epoch 5/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4620\n",
            "Epoch 6/20\n",
            "267/267==============================] - 71s 267ms/step - loss: 1.4567\n",
            "Epoch 7/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4524\n",
            "Epoch 8/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4503\n",
            "Epoch 9/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4470\n",
            "Epoch 10/20\n",
            "267/267==============================] - 71s 266ms/step - loss: 1.4501\n",
            "Epoch 11/20\n",
            "267/267==============================] - 70s 264ms/step - loss: 1.4471\n",
            "Epoch 12/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4523\n",
            "Epoch 13/20\n",
            "267/267==============================] - 71s 267ms/step - loss: 1.4509\n",
            "Epoch 14/20\n",
            "267/267==============================] - 72s 268ms/step - loss: 1.4537\n",
            "Epoch 15/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4613\n",
            "Epoch 16/20\n",
            "267/267==============================] - 71s 266ms/step - loss: 1.4638\n",
            "Epoch 17/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4683\n",
            "Epoch 18/20\n",
            "267/267==============================] - 71s 265ms/step - loss: 1.4747\n",
            "Epoch 19/20\n",
            "267/267==============================] - 71s 266ms/step - loss: 1.4819\n",
            "Epoch 20/20\n",
            "267/267==============================] - 70s 264ms/step - loss: 1.4898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Wo3urg2Dnvy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generating Text"
      ]
    },
    {
      "metadata": {
        "id": "LJ5AKQTRDr2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Restoring the latest checkpoint\n",
        "\n",
        "### Tensorflow note: \n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "metadata": {
        "id": "8VJI-tM_rpE-",
        "colab_type": "code",
        "outputId": "c40a8361-5179-4057-d93a-2fdaad902ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "AumXFz0tshZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwHucxamsl8A",
        "colab_type": "code",
        "outputId": "c4336abc-5294-46a9-8767-6c4dc9b61b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            19712     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 128)            147840    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 77)             9933      \n",
            "=================================================================\n",
            "Total params: 177,485\n",
            "Trainable params: 177,485\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dN7T7-ceOsra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The prediction loop\n",
        "\n",
        "## The following code block generates the text:\n",
        "\n",
        "### It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "### Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "### Then, use a multinomial distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "### The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words."
      ]
    },
    {
      "metadata": {
        "id": "Qm8M6mTZsrQD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 400\n",
        "\n",
        "  # You can change the start string to experiment\n",
        "  start_string = 'It is a truth univer'\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing) \n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66c9Ddzas-1O",
        "colab_type": "code",
        "outputId": "57f56cbf-0b38-441a-d84a-a5a841e31dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "cell_type": "code",
      "source": [
        "print(generate_text(model, start_string=\"Why, my dear, you mu\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-3884b09dee4a>:31: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "It is a truth univernes of his aspection, well, the same own hon he felt\n",
            "I could hape-ply reparing him into the leasure.”\n",
            "\n",
            "Elizabeth of there was. It mintinged. The who length; which and though that\n",
            "his partia here most an is unnewisture, you will an, eless. We could nature\n",
            "And the prodeigabld undest,\n",
            "as she were having to thinkinable monumple, or a compart it\n",
            "was the now prode cir\n",
            "this\n",
            "cothers prompt impromant of Mr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4iTDCdq9tC7L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}